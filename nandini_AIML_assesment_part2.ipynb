{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\khand\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 737us/step - loss: 16325.3945\n",
      "Epoch 2/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 714us/step - loss: 7583.7495\n",
      "Epoch 3/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 721us/step - loss: 6596.7236\n",
      "Epoch 4/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 712us/step - loss: 7080.6699\n",
      "Epoch 5/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 708us/step - loss: 5810.7559\n",
      "Epoch 6/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 676us/step - loss: 5685.0605\n",
      "Epoch 7/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 623us/step - loss: 7327.7935\n",
      "Epoch 8/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 643us/step - loss: 7903.2041\n",
      "Epoch 9/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 692us/step - loss: 6508.5591\n",
      "Epoch 10/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 705us/step - loss: 7235.1602\n",
      "Epoch 11/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 732us/step - loss: 8750.4043\n",
      "Epoch 12/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 689us/step - loss: 8682.7939\n",
      "Epoch 13/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 665us/step - loss: 11438.5469\n",
      "Epoch 14/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 682us/step - loss: 5325.7437\n",
      "Epoch 15/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 691us/step - loss: 9311.0576\n",
      "Epoch 16/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 683us/step - loss: 7152.7490\n",
      "Epoch 17/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 679us/step - loss: 9010.0107\n",
      "Epoch 18/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 666us/step - loss: 9031.7529\n",
      "Epoch 19/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 649us/step - loss: 10992.9561\n",
      "Epoch 20/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 716us/step - loss: 8262.2988\n",
      "Epoch 21/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 703us/step - loss: 10993.8047\n",
      "Epoch 22/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 662us/step - loss: 6340.1113\n",
      "Epoch 23/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 767us/step - loss: 5468.9868\n",
      "Epoch 24/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 768us/step - loss: 8462.0400\n",
      "Epoch 25/25\n",
      "\u001b[1m17193/17193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 700us/step - loss: 8429.1318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import joblib\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Training Dataset.csv')\n",
    "\n",
    "# Convert 'Month of Sourcing' column to datetime format\n",
    "train_data['Month of Sourcing'] = pd.to_datetime(train_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "train_data['Month of Sourcing'] = (train_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features and target variable\n",
    "X_train = train_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "y_train = train_data['Sourcing Cost']\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Reshape the input for LSTM (samples, timesteps, features)\n",
    "X_train_reshaped = X_train_scaled.reshape((X_train_scaled.shape[0], 1, X_train_scaled.shape[1]))\n",
    "\n",
    "# Initialize the LSTM model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# Add LSTM layer\n",
    "model_lstm.add(LSTM(units=50, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
    "\n",
    "# Add output layer\n",
    "model_lstm.add(Dense(units=1))\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Fit the LSTM model\n",
    "model_lstm.fit(X_train_reshaped, y_train, epochs=25, batch_size=32)\n",
    "\n",
    "# Save the trained model\n",
    "model_lstm.save('lstm_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step  \n",
      "Predicted Sourcing Cost (LSTM):\n",
      "[122.87993  136.90393  152.38939  121.44141  147.74292  160.7605\n",
      "  43.57338   56.13659  145.04924  139.47333  115.48572   70.59926\n",
      "  65.09178  103.95442  138.03671  141.28114  144.5171   141.98271\n",
      " 122.16222  130.08424  140.49763  112.34224  120.3725     5.686247\n",
      "  54.8114    26.557713  33.18408   38.351597  34.349365 199.05772\n",
      " 183.3354    77.496925  39.918625  26.876389  45.902176 122.074066\n",
      " 152.49416  135.62251  106.07156  192.70653   26.864319  13.109159\n",
      " 144.18861  156.38403   95.28659   37.977676 162.19402  182.4193\n",
      "  89.83034   74.88184   70.581436 127.791214  76.84285   43.071312\n",
      " 173.3926   111.81937  123.728516 195.35573  165.59435  117.107925\n",
      "  28.445017 119.95503  106.390945 139.46352  152.41185  125.62764\n",
      " 117.17116  146.2581   124.20471  135.30272  125.138275  99.65614\n",
      "  58.59089   24.909256  55.57221   44.439156  29.999046 113.18698\n",
      "  75.23874    6.920702  66.37496   36.90158   86.439865  76.690506\n",
      "  80.212494  56.195953  38.954536   8.563156  24.969131  42.532074\n",
      "  42.50323   55.525913 101.436966  72.32771   -9.362041  46.72213 ]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Test Dataset.csv')\n",
    "\n",
    "test_data['Month of Sourcing'] = pd.to_datetime(test_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "test_data['Month of Sourcing'] = (test_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features\n",
    "X_test = test_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Scale the features\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Reshape the input for LSTM (samples, timesteps, features)\n",
    "X_test_reshaped = X_test_scaled.reshape((X_test_scaled.shape[0], 1, X_test_scaled.shape[1]))\n",
    "\n",
    "# Load the trained LSTM model\n",
    "model_lstm = load_model('lstm_model.h5', compile= False)\n",
    "\n",
    "# Predict on the test set using the trained model\n",
    "y_pred_test_lstm = model_lstm.predict(X_test_reshaped)\n",
    "\n",
    "# Print the predicted sourcing costs\n",
    "print(\"Predicted Sourcing Cost (LSTM):\")\n",
    "print(y_pred_test_lstm.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost_model.pkl']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Training Dataset.csv')\n",
    "\n",
    "# Convert 'Month of Sourcing' column to datetime format\n",
    "train_data['Month of Sourcing'] = pd.to_datetime(train_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "train_data['Month of Sourcing'] = (train_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features and target variable\n",
    "X_train = train_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "y_train = train_data['Sourcing Cost']\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# Initialize and fit the XGBoost model\n",
    "model_xgb = xgb.XGBRegressor()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model_xgb, 'xgboost_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.011519 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 120\n",
      "[LightGBM] [Info] Number of data points in the train set: 550176, number of used features: 55\n",
      "[LightGBM] [Info] Start training from score 108.817286\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['lightgbm_model.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "\n",
    "# Load the training dataset\n",
    "train_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Training Dataset.csv')\n",
    "\n",
    "# Convert 'Month of Sourcing' column to datetime format\n",
    "train_data['Month of Sourcing'] = pd.to_datetime(train_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "train_data['Month of Sourcing'] = (train_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features and target variable\n",
    "X_train = train_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "y_train = train_data['Sourcing Cost']\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "\n",
    "# Initialize and fit the LightGBM model\n",
    "model_lgb = lgb.LGBMRegressor()\n",
    "model_lgb.fit(X_train, y_train)\n",
    "\n",
    "# Save the trained model\n",
    "joblib.dump(model_lgb, 'lightgbm_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sourcing Cost (XGBoost):\n",
      "[112.53496  153.59299  149.86044  145.43594  169.56429  173.7738\n",
      "  48.40108   66.15772  150.14774  149.32771  149.61943  133.9744\n",
      "  94.659904 141.45634  140.92635  151.98058  156.44974  142.05586\n",
      " 142.71555  149.45415  150.64648  147.1959   147.82664   17.159203\n",
      "  79.719734  32.06247   33.8921    30.179289  24.637184 216.41849\n",
      " 189.73964  170.93263   32.944557  25.22425   69.16799  150.20833\n",
      " 170.91917  145.28969  144.02716  183.52249   47.938313  35.7663\n",
      " 170.78976  173.05342  102.269516  41.178024 211.37619  212.7497\n",
      " 108.35295   72.27743   57.020947 149.30782   97.75054   26.040003\n",
      " 179.89725  125.62097  173.38013  199.1317   185.20116  143.21678\n",
      " 130.50778  148.262    148.03319  142.9592   143.99173  143.86136\n",
      " 143.62859  143.0835   145.05606  152.28763  148.02864  114.191795\n",
      "  72.61399   33.9718    63.71227   55.20187   25.908846 141.47995\n",
      "  93.2865     6.495615  71.57892   40.87667  158.1807   108.0876\n",
      "  70.43418  118.51166   67.01067   30.577536  27.66268   60.786407\n",
      "  53.061333  74.87136  123.76655   99.51181   12.353122  49.763363]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Test Dataset.csv')\n",
    "\n",
    "# Convert 'Month of Sourcing' column to datetime format\n",
    "test_data['Month of Sourcing'] = pd.to_datetime(test_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "test_data['Month of Sourcing'] = (test_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features\n",
    "X_test = test_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Load the trained XGBoost model\n",
    "model_xgb = joblib.load('xgboost_model.pkl')\n",
    "\n",
    "# Predict on the test set using the trained model\n",
    "y_pred_test_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Print the predicted sourcing costs\n",
    "print(\"Predicted Sourcing Cost (XGBoost):\")\n",
    "print(y_pred_test_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sourcing Cost (LightGBM):\n",
      "[111.87988611 153.69420111 153.38247842 144.37636632 168.57736791\n",
      " 171.05996081  53.90709126  61.67959004 146.19531602 146.19531602\n",
      " 146.81551464 134.7860545   95.93801217 142.12272701 142.12272701\n",
      " 149.20031568 150.31068258 144.41166615 143.19877539 150.12783321\n",
      " 150.40349601 145.67641876 145.67641876  22.03150276  79.14193391\n",
      "  32.90811574  23.42250899  30.79224679  25.30213589 219.20916952\n",
      " 185.65915776 166.96875094  31.77966854  24.58481237  66.86403181\n",
      " 150.19562697 160.77241912 145.8825077  144.09164729 181.37266234\n",
      "  53.53490778  41.86695227 171.11442223 171.1653136  104.85864409\n",
      "  40.77481057 210.87553401 211.72042837 107.01929289  70.70078709\n",
      "  60.98523072 148.5359255  106.78083172  97.33139226 182.70311984\n",
      " 134.29878018 165.4613988  202.90808825 196.4808534  144.54166503\n",
      " 139.05716957 148.98515753 147.28913734 144.49077366 144.54166503\n",
      " 146.73000725 144.71925046 145.92606548 145.59876408 151.52696144\n",
      " 147.38462235 106.30793635  68.73408857  42.25861106  57.84655362\n",
      "  54.35780916  28.49567603 134.66246061  90.21187871  17.46935237\n",
      "  67.93797752  39.50892765 154.2820115  107.26629145  67.98525535\n",
      " 119.95643979  67.13872067  32.73149054  32.16918643  69.73741335\n",
      "  52.14718822  71.35108889 120.19577736  97.78440178  27.20316142\n",
      "  51.22778997]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Load the test dataset\n",
    "test_data = pd.read_csv('DS_ML Coding Challenge Dataset (1).xlsx - Test Dataset.csv')\n",
    "\n",
    "# Convert 'Month of Sourcing' column to datetime format\n",
    "test_data['Month of Sourcing'] = pd.to_datetime(test_data['Month of Sourcing'], format='%b-%y')\n",
    "\n",
    "# Convert datetime to numeric representation (number of days since a reference date)\n",
    "test_data['Month of Sourcing'] = (test_data['Month of Sourcing'] - pd.Timestamp(\"1970-01-01\")) // pd.Timedelta('1D')\n",
    "\n",
    "# Extract features\n",
    "X_test = test_data[['ProductType', 'Manufacturer', 'Area Code', 'Sourcing Channel', 'Product Size', 'Product Type', 'Month of Sourcing']]\n",
    "\n",
    "# Convert categorical variables into dummy/indicator variables\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "# Load the trained LightGBM model\n",
    "model_lgb = joblib.load('lightgbm_model.pkl')\n",
    "\n",
    "# Predict on the test set using the trained model\n",
    "y_pred_test_lgb = model_lgb.predict(X_test)\n",
    "\n",
    "# Print the predicted sourcing costs\n",
    "print(\"Predicted Sourcing Cost (LightGBM):\")\n",
    "print(y_pred_test_lgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
